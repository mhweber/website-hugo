<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Marc Weber</title>
    <link>https://mhweber.github.io/post/</link>
    <description>Recent content in Posts on Marc Weber</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>mweber36@gmail.com (Marc Weber)</managingEditor>
    <webMaster>mweber36@gmail.com (Marc Weber)</webMaster>
    <lastBuildDate>Wed, 12 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mhweber.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Trying Out Bookdown and Publishing to GitHub Pages</title>
      <link>https://mhweber.github.io/2020/02/12/trying-out-bookdown-and-publishing-to-github-pages/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2020/02/12/trying-out-bookdown-and-publishing-to-github-pages/</guid>
      <description>I finally decided to try creating a bookdown project and see if I could figure out hosting it on GitHub Pages. The steps I followed were:
 Create a new empty GitHub repo on GitHub without a readme
 Create a new local R Project in a new directory as project type Book Project using bookdown
 Initialize a .git repository for the project after it is created
 Using code snippets on the GitHub repo you just created (initial page that shows up when you create the empty repository):</description>
    </item>
    
    <item>
      <title>Testing the rnassqs R package with Willamette basin data</title>
      <link>https://mhweber.github.io/2019/05/28/testing-the-rnassqs-r-package-with-willamette-basin-data/</link>
      <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2019/05/28/testing-the-rnassqs-r-package-with-willamette-basin-data/</guid>
      <description>Pull in data via API for Survey data This script uses the NASS API to query the NASS Quickstats service for historic acres harvested of primary crop groups in Willamette basin counties in Oregon from 1969-2013. An example of using the NASS Quickstats API to pull in and examine crop data in Willamette basin.
library(dplyr)  ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2  library(tibble) library(tidyr) library(rnassqs) library(lubridate) library(readr) years &amp;lt;- seq(as.</description>
    </item>
    
    <item>
      <title>Adding WMS basemap in R with mapview</title>
      <link>https://mhweber.github.io/2018/11/09/adding-wms-basemap-in-r-with-mapview/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2018/11/09/adding-wms-basemap-in-r-with-mapview/</guid>
      <description>Recently I wanted to be able to add a custom basemap of NHDPlus features to mapview leaflet maps. It&amp;rsquo;s not straitforward out of the box in mapview, but I found the helpful tips to do it here and here.
Below I add the WMS service from EPA Waters along with USGS StreamGage stations in Benton County, OR using the dataRetrieval package.
library(dataRetrieval) library(mapview) library(sf) library(leaflet) Stations &amp;lt;- readNWISdata(stateCd=&amp;quot;Oregon&amp;quot;, countyCd=&amp;quot;Benton&amp;quot;) # DataRetrival returns objects as &#39;attributes&#39; - things like the url used, site metadata, site info, etc - just use attributes(Durham_Stations) to examine siteInfo &amp;lt;- attr(Stations , &amp;quot;siteInfo&amp;quot;) stations_sf = st_as_sf(siteInfo, coords = c(&amp;quot;dec_lon_va&amp;quot;, &amp;quot;dec_lat_va&amp;quot;), crs = 4269,agr = &amp;quot;constant&amp;quot;) m &amp;lt;- mapview(stations_sf) m@map = m@map %&amp;gt;% addWMSTiles(group = &#39;NHDPlus&#39;, &amp;quot;https://watersgeo.</description>
    </item>
    
    <item>
      <title>R Spatial Tutorial</title>
      <link>https://mhweber.github.io/2018/09/12/spatial-tutorial/</link>
      <pubDate>Wed, 12 Sep 2018 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2018/09/12/spatial-tutorial/</guid>
      <description>Just finished putting together and running a half-day R Spatial Workshop covering some cool new R spatial things using packages such as sf, DataRetrieval, mapview, and combining dplyr chained operations with sf among other things. And I finally, after a year, got Disqus to start working on my blog! All I did was update blogdown and rebuild - if only everything were so simple&amp;hellip;</description>
    </item>
    
    <item>
      <title>Playing with Python Rasterstats</title>
      <link>https://mhweber.github.io/2017/10/13/playing-with-python-rasterstats/</link>
      <pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2017/10/13/playing-with-python-rasterstats/</guid>
      <description>This is code to get PctAg and PctUrb using NLCD 2006 for US Counties. First we&amp;rsquo;ll import some libraries.
import pandas as pd import numpy as np import geopandas as gp  Bring in US Counties using US Census counties shapefile
counties = gp.GeoDataFrame.from_file(&#39;L:/Priv/CORFiles/Geospatial_Library/Data/RESOURCE/POLITICAL/BOUNDARIES/NATIONAL/Counties_Census_2010.shp&#39;) list(counties) counties.STATE_NAME.unique() counties = counties[(counties.STATE_NAME != &#39;Hawaii&#39;) &amp;amp; (counties.STATE_NAME != &#39;Alaska&#39;)] counties = counties[[&#39;FIPS&#39;,&#39;NAME&#39;,&#39;geometry&#39;]] counties.head()    .dataframe thead tr:only-child th { text-align: right; } .</description>
    </item>
    
    <item>
      <title>GeoPandas Tutorial</title>
      <link>https://mhweber.github.io/2017/07/14/geopandas-tutorial/</link>
      <pubDate>Fri, 14 Jul 2017 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2017/07/14/geopandas-tutorial/</guid>
      <description>This is an implementation of the excellent PostGIS / geopandas tutorial here using NHDPlus WBD polygons for PNW. All the ideas and methods are from this tutorial, simply implementing with a different dataset and in Oregon.
%matplotlib inline import os import json import psycopg2 import matplotlib.pyplot as plt # The two statemens below are used mainly to set up a plotting # default style that&#39;s better than the default from matplotlib import seaborn as sns plt.</description>
    </item>
    
    <item>
      <title>Wordcloud</title>
      <link>https://mhweber.github.io/2017/07/13/wordcloud/</link>
      <pubDate>Thu, 13 Jul 2017 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2017/07/13/wordcloud/</guid>
      <description>I&amp;rsquo;ve been toying with building a wordcloud of all my publications in both R and python on and off for some time, and while there&amp;rsquo;s a really nice R library for doing this wordcloud2, this example uses wordcloud, a python word cloud generator.
I pasted all my publications into a single text file which is read into python and used by wordcloud as shown in code and results below.
from os import path from wordcloud import WordCloud # Read the whole text.</description>
    </item>
    
    <item>
      <title>Switching website from Jekyll to Hugo using Blogdown</title>
      <link>https://mhweber.github.io/2017/07/06/jekyll-to-hugo-with-blogdown/</link>
      <pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2017/07/06/jekyll-to-hugo-with-blogdown/</guid>
      <description>I set up my personal website using GitHub Pages a little while back using Jekyll and a Hyde template that I thought was pretty nice. However, after listening to Yihui&amp;rsquo;s RStudio webinar on using blogdown, I decided maybe it was time to give blogdown a try and switch from Jekyll to Hugo. I spent a good couple weeks off and on reading up on Hugo, exploring different Hugo themes, and building test websites and locally before finally feeling comfortable enough to take the plunge and make the switch.</description>
    </item>
    
    <item>
      <title>Test georasters</title>
      <link>https://mhweber.github.io/2017/05/05/test-georasters/</link>
      <pubDate>Fri, 05 May 2017 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2017/05/05/test-georasters/</guid>
      <description> Tried out the georasters python package for changing data types of rasters, recoding, merging, and writing out - very nice, handy python package!  </description>
    </item>
    
    <item>
      <title>Testing the simple features R package</title>
      <link>https://mhweber.github.io/2017/02/24/testing-the-simple-features-r-package/</link>
      <pubDate>Fri, 24 Feb 2017 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2017/02/24/testing-the-simple-features-r-package/</guid>
      <description>Load and look at basics of simple features package library(devtools) # install_github(&amp;quot;edzer/sfr&amp;quot;) library(sf)  ## Linking to GEOS 3.5.0, GDAL 2.1.1, proj.4 4.9.3  nc &amp;lt;- st_read(system.file(&amp;quot;shape/nc.shp&amp;quot;, package=&amp;quot;sf&amp;quot;))  ## Reading layer `nc&#39; from data source `C:\Users\mweber\R\library\sf\shape\nc.shp&#39; using driver `ESRI Shapefile&#39; ## converted into: MULTIPOLYGON ## Simple feature collection with 100 features and 14 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -84.32385 ymin: 33.88199 xmax: -75.</description>
    </item>
    
    <item>
      <title>Rasterize shapefiles in Python with rasterio and geopandas</title>
      <link>https://mhweber.github.io/2016/12/12/rasterize-shapefiles-in-python-with-rasterio-and-geopandas/</link>
      <pubDate>Mon, 12 Dec 2016 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2016/12/12/rasterize-shapefiles-in-python-with-rasterio-and-geopandas/</guid>
      <description> Here&amp;rsquo;s a snippet showing how to rasterize a shapefile in python using rasterio and geopandas:  </description>
    </item>
    
    <item>
      <title>Read PostGIS in R</title>
      <link>https://mhweber.github.io/2016/12/02/read-postgis-in-r/</link>
      <pubDate>Fri, 02 Dec 2016 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2016/12/02/read-postgis-in-r/</guid>
      <description> Working with Postgres / PostGIS more now and playing with reading and writing from R and Python. Here&amp;rsquo;s snippet showing read in of PostGIS in R:  </description>
    </item>
    
    <item>
      <title>Download StreamCat data</title>
      <link>https://mhweber.github.io/2016/09/20/download-streamcat-data/</link>
      <pubDate>Tue, 20 Sep 2016 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2016/09/20/download-streamcat-data/</guid>
      <description> Here&amp;rsquo;s an example code snippet using ftplib in python to download StreamCat data  </description>
    </item>
    
    <item>
      <title>R lookup</title>
      <link>https://mhweber.github.io/2016/03/10/r-lookup/</link>
      <pubDate>Thu, 10 Mar 2016 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2016/03/10/r-lookup/</guid>
      <description> Using match and indexing to create a lookup in R This code takes a lookup table and applies it to a data frame, updating only values for records that occur in the lookup table using indexing and match:
 </description>
    </item>
    
    <item>
      <title>R extract line endpoints</title>
      <link>https://mhweber.github.io/2016/02/02/r-extract-line-endpoints/</link>
      <pubDate>Tue, 02 Feb 2016 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/2016/02/02/r-extract-line-endpoints/</guid>
      <description>Extracting line end nodes in R
I noticed the maptools package in R had a SpatialLinesMidPoints function but couldn&amp;rsquo;t find any other out of the box functions in any packages to extract line endpoints. So I modified SpatialLinesMidPoints slightly to do the job:
 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://mhweber.github.io/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>mweber36@gmail.com (Marc Weber)</author>
      <guid>https://mhweber.github.io/1/01/01/</guid>
      <description>Trying Out Bookdown and Publishing to GitHub Pages/*! jQuery v1.11.3 | (c) 2005, 2015 jQuery Foundation, Inc. | jquery.org/license */!function(a,b){&#34;object&#34;==typeof module&amp;&amp;&#34;object&#34;==typeof module.exports?module.exports=a.document?b(a,!0):function(a){if(!a.document)throw new Error(&#34;jQuery requires a window with a document&#34;);return b(a)}:b(a)}(&#34;undefined&#34;!=typeof window?window:this,function(a,b){var c=[],d=c.slice,e=c.concat,f=c.push,g=c.indexOf,h={},i=h.toString,j=h.hasOwnProperty,k={},l=&#34;1.11.3&#34;,m=function(a,b){return new m.fn.init(a,b)},n=/^[\s\uFEFF\xA0]+|[\s\uFEFF\xA0]+$/g,o=/^-ms-/,p=/-([\da-z])/gi,q=function(a,b){return b.toUpperCase()};m.fn=m.prototype={jquery:l,constructor:m,selector:&#34;&#34;,length:0,toArray:function(){return d.call(this)},get:function(a){return null!=a?0a?this[a+this.length]:this[a]:d.call(this)},pushStack:function(a){var b=m.merge(this.constructor(),a);return b.prevObject=this,b.context=this.context,b},each:function(a,b){return m.each(this,a,b)},map:function(a){return this.pushStack(m.map(this,function(b,c){return a.call(b,c,b)}))},slice:function(){return this.pushStack(d.apply(this,arguments))},first:function(){return this.eq(0)},last:function(){return this.eq(-1)},eq:function(a){var b=this.length,c=+a+(0a?b:0);return this.pushStack(c=0&amp;&amp;bc?[this[c]]:[])},end:function(){return this.prevObject||this.constructor(null)},push:f,sort:c.sort,splice:c.splice},m.extend=m.fn.extend=function(){var a,b,c,d,e,f,g=arguments[0]||{},h=1,i=arguments.length,j=!1;for(&#34;boolean&#34;==typeof g&amp;&amp;(j=g,g=arguments[h]||{},h++),&#34;object&#34;==typeof g||m.isFunction(g)||(g={}),h===i&amp;&amp;(g=this,h--);ih;h++)if(null!=(e=arguments[h]))for(d in e)a=g[d],c=e[d],g!==c&amp;&amp;(j&amp;&amp;c&amp;&amp;(m.isPlainObject(c)||(b=m.isArray(c)))?(b?(b=!1,f=a&amp;&amp;m.isArray(a)?a:[]):f=a&amp;&amp;m.isPlainObject(a)?a:{},g[d]=m.extend(j,f,c)):void 0!==c&amp;&amp;(g[d]=c));return g},m.extend({expando:&#34;jQuery&#34;+(l+Math.random()).replace(/\D/g,&#34;&#34;),isReady:!0,error:function(a){throw new Error(a)},noop:function(){},isFunction:function(a){return&#34;function&#34;===m.type(a)},isArray:Array.isArray||function(a){return&#34;array&#34;===m.type(a)},isWindow:function(a){return null!=a&amp;&amp;a==a.window},isNumeric:function(a){return!m.isArray(a)&amp;&amp;a-parseFloat(a)+1=0},isEmptyObject:function(a){var b;for(b in a)return!1;return!0},isPlainObject:function(a){var b;if(!a||&#34;object&#34;!==m.type(a)||a.nodeType||m.isWindow(a))return!1;try{if(a.constructor&amp;&amp;!j.call(a,&#34;constructor&#34;)&amp;&amp;!j.call(a.constructor.prototype,&#34;isPrototypeOf&#34;))return!1}catch(c){return!1}if(k.ownLast)for(b in a)return j.call(a,b);for(b in a);return void 0===b||j.</description>
    </item>
    
  </channel>
</rss>